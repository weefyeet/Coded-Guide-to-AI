In language modelling, several options exist for word processing. Most of these involve neural networks, incorporating various concepts that differ in their optimization of language modelling. 

RNNs (recurrent neural networks) for instance are sequentially dependent. RNNs read through language in order, unable to process a word until the preceding word is processed. When the order of words is relevant or certain temporal dependencies exist, RNNs fare relatively well; however, for long sequences of words, the sequential processing can result in significant bottlenecks. Furthermore, an RNN's ability to remember information decreases as dependencies farther in distance lose context value due to vanishing gradients.

CNNs (convolutional neural networks) are not sequentially dependent. CNNs read through information through kernels, which examine information in a limited size defined by the kernel. The operation between the kernel and the section of information read produces values on a feature map to indicate patterns detected. Early layer kernels focus on simpler pattern concepts, whereas deeper layer kernels recognize more complex patterns. Because various kernel convolutions are not sequentially dependent ulike RNNs, these kernel convolutions can be parallelized, which mean many such operations can be run simultaneously, eliminating the bottlenecks observed in RNNs working with larger word sequences. On the other hand, since kernels examine kernel sized window of information, any dependencies outside of the kernel window will not be captured without sufficiently stacked layers, at which point CNNs lose their efficiency advantages. 

The transformer architecture thus tackles the limitations of recurrence and convolutional models with the self-attention mechanism, which weights connections between each token in a sequence with every other token. Self-attention addresses the deficiencies presented by RNNs and CNNs simultaneously. While RNNs rely on sequential dependency for context, self-attention means transformers can parallelize sequence processing, eliminating the RNN bottlenecks. Positional encoders are employed to keep track of token positions instead. Similarly, CNNs struggle to form dependencies between distant data without stacking layers and reducing efficiency, but the interconnected nature of self-attention enables dependencies to be analyzed without regard for distance between data. However, due to the quadratic complexity of self-attention, transformer computation becomes very expensive with large sequences, limiting sequence length, batch size, and training speed.

