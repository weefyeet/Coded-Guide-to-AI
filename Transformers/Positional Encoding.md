Positional encoding is an essential concept in transformer architecture. Transformers process all tokens in parallel, which means that unlike RNNs, which process tokens sequentially, transformers have no sense of order with regards to information, rendering the tokens into a rather chaotic "bag" of words. Since word order is vital to context and meaninng in language, positional encoding gives structure and order to the tokens, thus allowing the model to capture order-dependent relationships betweentokens. Because positional encoding vectors have the same dimensionality as token embedding vectors, the information can be directly combined to represent both the token and its position. Self-attention can then consider position alongisde the token content whe weighting between tokens.
